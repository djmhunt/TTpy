20-03-03 18:58 Setup        INFO     2020-3-3
20-03-03 18:58 Setup        INFO     Log initialised
20-03-03 18:58 Setup        INFO     The log you are reading was written to D:/Dropbox/Dom/PhD/Models/Code/runScripts/Outputs/qLearn_probSelectSimSet_2020-3-3/log.txt
20-03-03 18:58 Framework    INFO     Beginning task labelled: qLearn_probSelectSimSet
20-03-03 18:58 Simulation   INFO     Simulation 0 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.1, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 0
20-03-03 18:58 Simulation   INFO     Simulation 1 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.1, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 1
20-03-03 18:58 Simulation   INFO     Simulation 2 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.1, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 2
20-03-03 18:58 Simulation   INFO     Simulation 3 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.1, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 3
20-03-03 18:58 Simulation   INFO     Simulation 4 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.1, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 4
20-03-03 18:58 Simulation   INFO     Simulation 5 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.1, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 5
20-03-03 18:58 Simulation   INFO     Simulation 6 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.5, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 6
20-03-03 18:58 Simulation   INFO     Simulation 7 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.5, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 7
20-03-03 18:58 Simulation   INFO     Simulation 8 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.5, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 8
20-03-03 18:58 Simulation   INFO     Simulation 9 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.5, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 9
20-03-03 18:58 Simulation   INFO     Simulation 10 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.5, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 10
20-03-03 18:58 Simulation   INFO     Simulation 11 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 0.5, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 11
20-03-03 18:58 Simulation   INFO     Simulation 12 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 1.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 12
20-03-03 18:58 Simulation   INFO     Simulation 13 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 1.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 13
20-03-03 18:58 Simulation   INFO     Simulation 14 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 1.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 14
20-03-03 18:58 Simulation   INFO     Simulation 15 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 1.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 15
20-03-03 18:58 Simulation   INFO     Simulation 16 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 1.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 16
20-03-03 18:58 Simulation   INFO     Simulation 17 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 1.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 17
20-03-03 18:58 Simulation   INFO     Simulation 18 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 2.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 18
20-03-03 18:58 Simulation   INFO     Simulation 19 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 2.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 19
20-03-03 18:58 Simulation   INFO     Simulation 20 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 2.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 20
20-03-03 18:58 Simulation   INFO     Simulation 21 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 2.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 21
20-03-03 18:58 Simulation   INFO     Simulation 22 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 2.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 22
20-03-03 18:58 Simulation   INFO     Simulation 23 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 2.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 23
20-03-03 18:58 Simulation   INFO     Simulation 24 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 4.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 24
20-03-03 18:58 Simulation   INFO     Simulation 25 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 4.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 25
20-03-03 18:58 Simulation   INFO     Simulation 26 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 4.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 26
20-03-03 18:58 Simulation   INFO     Simulation 27 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 4.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 27
20-03-03 18:58 Simulation   INFO     Simulation 28 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 4.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 28
20-03-03 18:58 Simulation   INFO     Simulation 29 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 4.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 29
20-03-03 18:58 Simulation   INFO     Simulation 30 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 8.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 30
20-03-03 18:58 Simulation   INFO     Simulation 31 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 8.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 31
20-03-03 18:58 Simulation   INFO     Simulation 32 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 8.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 32
20-03-03 18:58 Simulation   INFO     Simulation 33 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 8.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 33
20-03-03 18:58 Simulation   INFO     Simulation 34 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 8.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 34
20-03-03 18:58 Simulation   INFO     Simulation 35 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 8.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 35
20-03-03 18:58 Simulation   INFO     Simulation 36 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 16.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 36
20-03-03 18:58 Simulation   INFO     Simulation 37 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 16.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.1, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 37
20-03-03 18:58 Simulation   INFO     Simulation 38 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 16.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 38
20-03-03 18:58 Simulation   INFO     Simulation 39 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 16.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.5, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 39
20-03-03 18:58 Simulation   INFO     Simulation 40 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 16.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 40
20-03-03 18:58 Simulation   INFO     Simulation 41 contains the task ProbSelect: reward_probability = 0.7, reward_size = 1, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], action_reward_probabilities = {'A': 0.8, 'C': 0.7, 'B': 0.2, 'E': 0.6, 'D': 0.3, 'F': 0.4}, test_length = 100, learning_length = 200, number_actions = 6.The model used is QLearn: number_cues = 1, reward_shaper = u'probSelect.RewardProbSelectDirect with Name : probSelect.RewardProbSelectDirect', stimulus_shaper = u'probSelect.StimulusProbSelectDirect with Name : probSelect.StimulusProbSelectDirect', number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]), decision_function = u"discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", beta = 16.0, actionCode = {'A': 0, 'C': 2, 'B': 1, 'E': 4, 'D': 3, 'F': 5}, non_action = u'None', alpha = 0.9, number_actions = 6.
20-03-03 18:58 Framework    INFO     Beginning simulation output processing
20-03-03 18:58 Framework    INFO     Store data for simulation 41
20-03-03 18:58 Setup        INFO     Shutting down program
