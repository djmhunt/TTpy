20-05-08 21:51 Setup        INFO     2020-5-8
20-05-08 21:51 Setup        INFO     Log initialised
20-05-08 21:51 Setup        INFO     The log you are reading was written to test_sim/log.txt
20-05-08 21:51 Framework    INFO     Beginning task labelled: qLearn_probSelectSimSet
20-05-08 21:51 Simulation   INFO     Simulation 0 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.1, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 1 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.1, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 2 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.1, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 3 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.5, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 4 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.5, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 5 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.5, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 6 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 1.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 7 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 1.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 8 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 1.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 9 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 4.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 10 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 4.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 11 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 4.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 12 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 16.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 13 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 16.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Simulation   INFO     Simulation 14 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = {'A': 0.8, 'B': 0.2, 'C': 0.7, 'D': 0.3, 'E': 0.6, 'F': 0.4}, learning_action_pairs = [['A', 'B'], ['C', 'D'], ['E', 'F']], learning_length = 100, test_length = 50, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 16.0, expectation = array([[0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667],
       [0.16666667]]).
20-05-08 21:51 Setup        INFO     Shutting down program
