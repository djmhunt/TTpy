20-03-22 20:18 Setup        INFO     2020-3-22
20-03-22 20:18 Setup        INFO     Log initialised
20-03-22 20:18 Setup        INFO     The log you are reading was written to test_sim/log.txt
20-03-22 20:18 Framework    INFO     Beginning task labelled: qLearn_probSelectSimSet
20-03-22 20:18 Simulation   INFO     Simulation 0 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 0
20-03-22 20:18 Simulation   INFO     Simulation 1 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 1
20-03-22 20:18 Simulation   INFO     Simulation 2 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 2
20-03-22 20:18 Simulation   INFO     Simulation 3 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 3
20-03-22 20:18 Simulation   INFO     Simulation 4 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 4
20-03-22 20:18 Simulation   INFO     Simulation 5 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 5
20-03-22 20:18 Simulation   INFO     Simulation 6 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 6
20-03-22 20:18 Simulation   INFO     Simulation 7 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 7
20-03-22 20:18 Simulation   INFO     Simulation 8 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 8
20-03-22 20:18 Simulation   INFO     Simulation 9 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.1, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 9
20-03-22 20:18 Simulation   INFO     Simulation 10 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 10
20-03-22 20:18 Simulation   INFO     Simulation 11 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 11
20-03-22 20:18 Simulation   INFO     Simulation 12 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 12
20-03-22 20:18 Simulation   INFO     Simulation 13 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 13
20-03-22 20:18 Simulation   INFO     Simulation 14 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 14
20-03-22 20:18 Simulation   INFO     Simulation 15 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 15
20-03-22 20:18 Simulation   INFO     Simulation 16 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 16
20-03-22 20:18 Simulation   INFO     Simulation 17 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 17
20-03-22 20:18 Simulation   INFO     Simulation 18 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 18
20-03-22 20:18 Simulation   INFO     Simulation 19 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.3, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:18 Framework    INFO     Beginning simulation output processing
20-03-22 20:18 Framework    INFO     Store data for simulation 19
20-03-22 20:18 Simulation   INFO     Simulation 20 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 20
20-03-22 20:19 Simulation   INFO     Simulation 21 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 21
20-03-22 20:19 Simulation   INFO     Simulation 22 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 22
20-03-22 20:19 Simulation   INFO     Simulation 23 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 23
20-03-22 20:19 Simulation   INFO     Simulation 24 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 24
20-03-22 20:19 Simulation   INFO     Simulation 25 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 25
20-03-22 20:19 Simulation   INFO     Simulation 26 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 26
20-03-22 20:19 Simulation   INFO     Simulation 27 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 27
20-03-22 20:19 Simulation   INFO     Simulation 28 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 28
20-03-22 20:19 Simulation   INFO     Simulation 29 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.5, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 29
20-03-22 20:19 Simulation   INFO     Simulation 30 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 30
20-03-22 20:19 Simulation   INFO     Simulation 31 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 31
20-03-22 20:19 Simulation   INFO     Simulation 32 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 32
20-03-22 20:19 Simulation   INFO     Simulation 33 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 33
20-03-22 20:19 Simulation   INFO     Simulation 34 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 34
20-03-22 20:19 Simulation   INFO     Simulation 35 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 35
20-03-22 20:19 Simulation   INFO     Simulation 36 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 36
20-03-22 20:19 Simulation   INFO     Simulation 37 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 37
20-03-22 20:19 Simulation   INFO     Simulation 38 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 38
20-03-22 20:19 Simulation   INFO     Simulation 39 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 0.7, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 39
20-03-22 20:19 Simulation   INFO     Simulation 40 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 40
20-03-22 20:19 Simulation   INFO     Simulation 41 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 41
20-03-22 20:19 Simulation   INFO     Simulation 42 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 42
20-03-22 20:19 Simulation   INFO     Simulation 43 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 43
20-03-22 20:19 Simulation   INFO     Simulation 44 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 44
20-03-22 20:19 Simulation   INFO     Simulation 45 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 45
20-03-22 20:19 Simulation   INFO     Simulation 46 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 46
20-03-22 20:19 Simulation   INFO     Simulation 47 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 47
20-03-22 20:19 Simulation   INFO     Simulation 48 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 48
20-03-22 20:19 Simulation   INFO     Simulation 49 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 1.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 49
20-03-22 20:19 Simulation   INFO     Simulation 50 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 50
20-03-22 20:19 Simulation   INFO     Simulation 51 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 51
20-03-22 20:19 Simulation   INFO     Simulation 52 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 52
20-03-22 20:19 Simulation   INFO     Simulation 53 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 53
20-03-22 20:19 Simulation   INFO     Simulation 54 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 54
20-03-22 20:19 Simulation   INFO     Simulation 55 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 55
20-03-22 20:19 Simulation   INFO     Simulation 56 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 56
20-03-22 20:19 Simulation   INFO     Simulation 57 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 57
20-03-22 20:19 Simulation   INFO     Simulation 58 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 58
20-03-22 20:19 Simulation   INFO     Simulation 59 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 2.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 59
20-03-22 20:19 Simulation   INFO     Simulation 60 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 60
20-03-22 20:19 Simulation   INFO     Simulation 61 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 61
20-03-22 20:19 Simulation   INFO     Simulation 62 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 62
20-03-22 20:19 Simulation   INFO     Simulation 63 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 63
20-03-22 20:19 Simulation   INFO     Simulation 64 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 64
20-03-22 20:19 Simulation   INFO     Simulation 65 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 65
20-03-22 20:19 Simulation   INFO     Simulation 66 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 66
20-03-22 20:19 Simulation   INFO     Simulation 67 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 67
20-03-22 20:19 Simulation   INFO     Simulation 68 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 68
20-03-22 20:19 Simulation   INFO     Simulation 69 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 4.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 69
20-03-22 20:19 Simulation   INFO     Simulation 70 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 70
20-03-22 20:19 Simulation   INFO     Simulation 71 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 71
20-03-22 20:19 Simulation   INFO     Simulation 72 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 72
20-03-22 20:19 Simulation   INFO     Simulation 73 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 73
20-03-22 20:19 Simulation   INFO     Simulation 74 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 74
20-03-22 20:19 Simulation   INFO     Simulation 75 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 75
20-03-22 20:19 Simulation   INFO     Simulation 76 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 76
20-03-22 20:19 Simulation   INFO     Simulation 77 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 77
20-03-22 20:19 Simulation   INFO     Simulation 78 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 78
20-03-22 20:19 Simulation   INFO     Simulation 79 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 8.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 79
20-03-22 20:19 Simulation   INFO     Simulation 80 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 80
20-03-22 20:19 Simulation   INFO     Simulation 81 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.1, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 81
20-03-22 20:19 Simulation   INFO     Simulation 82 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 82
20-03-22 20:19 Simulation   INFO     Simulation 83 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.3, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 83
20-03-22 20:19 Simulation   INFO     Simulation 84 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 84
20-03-22 20:19 Simulation   INFO     Simulation 85 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.5, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 85
20-03-22 20:19 Simulation   INFO     Simulation 86 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 86
20-03-22 20:19 Simulation   INFO     Simulation 87 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.7, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 87
20-03-22 20:19 Simulation   INFO     Simulation 88 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 88
20-03-22 20:19 Simulation   INFO     Simulation 89 contains the task ProbSelect: reward_probability = 0.7, action_reward_probabilities = OrderedDict([('A', 0.8), ('B', 0.2), ('C', 0.7), ('D', 0.3), ('E', 0.6), ('F', 0.4)]), learning_action_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F')], learning_length = 200, test_length = 100, number_actions = 6, reward_size = 1.The model used is QLearn: number_actions = 6, number_cues = 1, number_critics = 6, prior = array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667]), non_action = 'None', actionCode = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}, stimulus_shaper = 'tasks.probSelect.StimulusProbSelectDirect with ', reward_shaper = 'tasks.probSelect.RewardProbSelectDirect with ', decision_function = "discrete.weightProb with task_responses : 'A', 'B', 'C', 'D', 'E', 'F'", alpha = 0.9, beta = 16.0, expectation = array([[0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5],
       [0.5]]).
20-03-22 20:19 Framework    INFO     Beginning simulation output processing
20-03-22 20:19 Framework    INFO     Store data for simulation 89
20-03-22 20:19 Setup        INFO     Shutting down program
